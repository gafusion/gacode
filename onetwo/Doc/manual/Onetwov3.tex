  \documentclass[12pt]{article} 
   \input{abbrev} % can keep my definitions is file abbrev.tex
   \usepackage{epsfig}
    \begin{document} 
      %\title{  (title gets put on separate page)
\begin{center} \bfseries
  \ot Version 3.0 Release Notes \\
  H. St.John, 03/04/03 \\
\end{center}
    %  }
        \section*{Summary}
        %\Large \boldmath 
        \normalsize \mdseries
     Recent changes to \ot are described in sufficient detail to allow
     the user community to apply  them without ambiguity.
     This includes interface changes to version 1.6 of Toray,
     version 3.0 of Preplt, and version 1.6 of Glf23.
     The time  dependent eqdsk mode of operation (TDEM)  is incoporated
     into the Toray interface and the settup of TDEM operation is
     described. Using  the new Glf23 confinement model in
     \ot is described along with a brief description of the possible
     solution methods. Some results for shot 111221 are given as an
     example application.

  
   \section{Toray Interface} 
    Toray can be run both as a stand alone code as well as a 
    slave process controlled by \ot. In either  case Toray requires
    at least the input file mhddat before it can be run.
    Namelist files, toray.in and
    gafit.in  are 
    required in some circumstances  and the kinetic data file, echin,
    is required before file mhddat can be generated. The file echin is
    generally obtained by running \ot even if toray is to be run in
    stand alone mode. The mhddat file is created by Gafit. In versions
    of Toray  prior to version 1.4 the only way that
    Gafit could generate the mhddat file was by reading the mhd
    information from an input file,psiin, generated by \ot.
     Starting with version 1.4 of Toray, Gafit is no longer an indepedent code.
     Instead, the functionality of Gafit has been absorbed in Toray.
     This requires that Onetwo must spawn Toray directly if Toray
     v. 1.4 or greater is used or \ot must spawn Gafit and then spawn Toray if
     a version of Toray prior to version 1.4 is used. Since it appears
     that there may be use for situatons where gafit is run
     independently of Toray, even for Toray versions that have a gafit
     built into them, /ot lets the user decide which option to use.

     To spawn Toray directly requires that Onetwo
     must create the file toray.in with appropriate information in it
     or, alternatively, the user can create toray.in before running
     \ot. The kinetic data file, echin, must still be created by \ot.
     The file psiin can still be created by \ot, or alternatively,
     an eqdsk file, which must be  called eqskin, can be
     read by the internal Gafit
     section of Toray 1.4 and the file psiin generated that way.
     The decision as to which way psiin is generated  is made by setting
     ipsi =0 or 1 in input file gafit.in. 
 
     Assuming that we are running Toray v1.4 or greater as an \ot spawned
     process we have to set switch igafit in toray.in to 0 or 1 to indicate
     that file mhddat is to be generated by gafit or just read in
     because  it allready exists. The default value of igafit is
     0 .

     A number of different equilibrium and transport grid sizes 
     have arisen over the past several years. Since neither Toray nor
     \ot are dynamically allocated we need to have some way of
     determining what the relevant grid sizes are that each code is
     compiled with. The convention choosen for \ot is that the code
     name is followed by the mhd grid size and then the transport grid
     size. For square mhd grids the second mhd dimension is left off.
     hence we have onetwo\_129\_51, onetwo\_65\_129\_51, onetwo\_65\_201,
     etc.(the last number is always the radial transport grid size)
      The toray and gafit versions linked by \ot are 
     commensurate with this scheme and  stored in palaces that the
     \ot code knows about. The user can take control of which version
     of Toray should be run by \ot,see below. Alternatively \ot will
     use the latest version of Toray that it knows about.
     The version of Toray run will is  printed out 
     in file outone as well as to  the terminal screen).

  \section{Implementation}
     The default setting of switch ipsi in gafit.in  for Toray 1.6 is
     0. This is the correct value for spawing Toray from \ot and hence
     gafit.in is not required unless the user has some other reason
     to supply it. If gafit.in is supplied then ipsi =0 must be set.
     \ot WILL FORCE THE CORRECT SETTING BY MODIFYING THE USERS
     GAFIT.IN IF NECESSARY. The default setting of igafit is 0, which
     is not appropriate for spawning Toray from \ot. Hence a toray.in
     file must be created by \ot if the user did not supply one. If
     the user did supply a toray.in file then that file is checked to
     make sure that igafit=1 is set. If is is not \ot WILL FORCE THE
     CORRECT SETTING BY MODIFYING THE USERS toray.in file. (actually
     both gafit.in and toray.in will be copied to gafit.in\_user
     and toray.in\_user and new copies with the switches ipsi and
     igafit set correctly will be generated).


     The following new switches must be
     set in the second namelist of \ot in order to run Toray as a
     slave process. This additional  input applies to all versions of
     \ot.

     \begin{verbatim}
     $namelis2
      ...     beam input, etc. 
      toray_version = 0.97
      toray_path  =
      echin_save  =  0
      irfcur(i) = 1.0 , 1.5, etc now a flaoting point number
      ...     rest of namelist
     $end
    \end{verbatim}


       Plot12 now has heating and current drive for the
       various rf cases broken out individually. Since there is some
       question about current drive efficiency the values of the
       current drive multiplier,irfcur,
       are now floating point nubers instead of integers as
       noted in the above namelist segement. This allows application
       of the full heating power, independent of the amount of
       current drive that is wanted.
     A description of the new input variables is given in cray102.f
     For convenience the descriptions  are also repeated below.

         There are three ways to spawn a version of Toray.
         The first is to just rely on the default settings in
         Onetwo (see default setting of the switches). This way
         should almost always be what the user wants.
         The second is to specify that a particular version
         of Toray is to be run,using switch toray\_version. Onetwo will
         search its Toray paths on the architecture it is running on 
         and if an appropriate version with the right mhd and transport grid
         sizes is found, it will be used. If this fails the user
         will be informed and the code will quit.( It is becoming quite
         challenging to keep up with all Toaty versions, grid sizes, 
         and architectures. Hence users that require soem special version/
         architecture may specifically have to request it).
         The third way is to specify a path to a custom built Toray.
         (This allows the latest  versions of Toray to be used before
         they become public for example). Set toray\_path to the complete
         executable name. For example( on Hydra): 
         /u/stjohn/toray/toray/129\_51/hp/mytoray.
         This will force Onetwo to use the executable called mytoray in
         /u/stjohn/toray/toray/129\_51/hp
         For your custom toray you must be sure that the Onetwo and
         Toray grid sizes are commensurate. The set of standard
         sizes for \ot are  (65x65,51), (129x129,51), other
         standard sizes will be defined as necessary or requested.
         Note that mytoray above is not restricted in any way. You
         could for example put a script in file mytoray (which must
         be executable) that goes to some remote machine, does some
         toray like calculations and returns the results in file
         echout that onetwo knows how to read.


         Toray\_path, if set, takes precedence. If not set (in inone)
         then toray\_version is checked. If this is not set then
         the default mode is used.
         The prebuilt versions of Toray that \ot has access to are
         given in file //
         /u/stjohn/onetwo\_nubeam/set\_rf\_data.f90  //

         For reference that file is listed here (but it will probbly change).
         \begin{verbatim}

       subroutine set_rf_data
!  this pculiar way of initalizations was forced upon me by the pg90
!  compiler -HSJ
         USE rf_info
             host_names =                         &
                  (/'TAURUS','lohan1','lohan2','HYDRA ', &
                              'delphi','cardea','katze '/)
             !it should be fairly obvious  how to modify this
             !the assumed properties (in sub get_toray) are
             !given here:
             ! (1) all hosts must have n_versions  versions available:
              versions = (/ 0.97,1.41 /) 

             !(2) last three fields in toray_paths 
             !(ie fileds after root_str) must follow the
             !root_str(j)//'/vx.xx/grid/toray' 
             !pattern otherwise sub get_toray will fail !!!!!!
             !(3) obviously if any of the parameters 
             !ncpu_arch,n_versions,grid_types are 
             !changed then the following data statements must be
             !changed accordingly. 
             !(I do not have  the luxury of creating code generators
             !to do such things - HSJ )


             !taurus data:
             toray_paths(1,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(1,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(1,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(1,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray'  
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray'  
             root_str(1) = '/usr/local/bin/toray' 


             !lohan1 data:
             toray_paths(2,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(2,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(2,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(2,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray' 
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray' 
             root_str(2)  = '/usr/local/bin/toray' 


             !lohan3 data:
             toray_paths(3,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(3,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(3,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(3,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray'
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray'   
             root_str(3) = '/usr/local/bin/toray' 

             !hydra data:
             toray_paths(4,1,1)= '/u/stjohn/toray/v1.41/129_51/toray'
             toray_paths(4,2,1)= '/u/stjohn/toray/v0.97/129_51/toray' 
             toray_paths(4,1,2)= '/u/stjohn/toray/v1.41/65_51/toray'
             toray_paths(4,2,2)= '/u/stjohn/toray/v0.97/65_51/toray' 
             toray_paths(1,1,3)= '/u/stjohn/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/u/stjohn/toray/v0.97/129_201/toray' 
             root_str(4) = '/u/stjohn/toray/' 

             !delphi data:
             toray_paths(5,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(5,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(5,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(5,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray'
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray'   
             root_str(5) = '/usr/local/bin/toray' 

             !cardea data:
             toray_paths(6,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(6,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(6,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(6,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray'
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray'  
             root_str(6) = '/usr/local/bin/' 

             !katze data:
             toray_paths(7,1,1)= '/usr/local/bin/toray/v1.41/129_51/toray'
             toray_paths(7,2,1)= '/usr/local/bin/toray/v0.97/129_51/toray' 
             toray_paths(7,1,2)= '/usr/local/bin/toray/v1.41/65_51/toray'
             toray_paths(7,2,2)= '/usr/local/bin/toray/v0.97/65_51/toray'
             toray_paths(1,1,3)= '/usr/local/bin/toray/v1.41/129_201/toray'
             toray_paths(1,2,3)= '/usr/local/bin/toray/v0.97/129_201/toray'  
             root_str(7)  = '/usr/local/bin/toray' 
         end subroutine set_rf_data

         \end{verbatim}

         you can check the build  date in these directories to find out
         what the latest build is.
 

         Issues involved with tdem mode
         of operation of Onetwo are handled automatically if the user
         has choosen to run  TDEM mode.
 
     \begin{description}
       \item {gafsep}  This quantity was previously passed to Toray
         with a fixed value of 1.e-6. The user selected input vaue was
         ignored. It is now passed to toray as expected. The default
         has been set to 1.e-6 .
       \item [toray\_version]  default is the  most recent
         version that Onetwo knows about. This information is keept in
         file ext\_prog\_info.f90 mentioned above. Specify a
         number . gt. 1.3 for new Toray f90 version with an internal
         gafit . Specify a 
         number less than 1.3  to get the old version of toray with
         gafit run as a separate program.
       \item [toray\_path]  The default path is set internally to
         point at the selected version of toray. If you  want to run a
         specific version then you can set the path to that version
         here. If toray\_path is set you  must also set
         toray\_version  (so that \ot can select the proper interface)!
         Note that if you use gafit.in and/or toray.in then these
         files must be commensurate with the version of toray
         that you specified in toray\_path !
         toray\_path is a 256 (or less) character variable. 
         \ot  spawns gafit and toray as separate processes for
          versions less than 1.3 and spawns just the toray
          process otherwise. Note that no attempt is made to do
          remote procedure calls so that a path pointing
          to a dfferent machine other than the one that Onetwo is
          running on is not allowed. 
       \item [echin\_save]  integer, either 0 or 1, default 0. 
         Toray can be run in
         standalone mode using just the files echin and mhddat. Hence
         if the files echin and mhddat  are saved as Onetwo executes it is
         possible to go back and rerun toray (perhaps changing 
         something in the echin file). echin\_save  =0 retains only the
         last version of echin,mhddat created by \ot. echin\_save =1 saves
         all versions of these files indexed by a time stamp. Notice
         that if you have a time sequence of eqdsks processed using
         TDEM  mode then this option is a convenient way to get time
         interpolated mhddat files. Note however that mhddat is a
         binary file which means its non portable across
         machines.( A netcdf file would make more sense here I think
         but thats a Toray issue). Finally recall that
         Toray can be run using only the input files echin and mhddat
         only if toray.in has igafit =0 (which is the default in the
         Toray v1.4  code).

     \end{description}

     Note that mhddat is a binary file hence it cannot be read  on
     a different machine architecture than it was written on.
     \section{MEPC Code}
       The multiple eqdsk processor code (MEPC)  must be run 
       before a Onetwo tdem run can be done. The MEPC code
       takes as input a list of up to kbctim = 150 (parameter kbctim is
       defined in param.f90) time evolved eqdsks and generates a single
       netcdf output file. Please note that Onetwo requires that at
       least 3 eqdsks are
       in the netcdf file.  The  netcdf file name  is then input into the
       third namelist of inone in place of an eqdsk name as explained
       above. MEPC will print out instructions on how to use it if you
       execute the code without any command line arguments. The
       required arguments are listed by the code and hence they are
       not repeated here. The list of eqdsk that MEPC will process is
       given in a file (with a name specified as one of the command
       line arguments. For definitenes we will assume the file is
       called eqdsk.list here). Eqdsk.list contains a list of eqdsks that
       will be used to generate the netcdf output file. The Eqdsk.list
       file can be created  by a gui program,
       MEPC.tcl  or, alternatively, this file can be created manually
       with a text editor of your choice.
       When reading the Eqdsk.lsit file the MEPC code scans each line
       in the file for special sentinels. Any line starting with
       \#,;, or ! will be ignored. Valid input lines that specify
       eqdisks that cant be found are skipped over. The sentinel 
       END on a line by itself will terminate the reading of the
       Eqdsk.list file. Eqdsks that are smaller than the compiled
       in size of the code (currently 129 by 129 in R,Z)
    \begin{verbatim}
c  namelist mepcinput:
c  time_start              Use only those eqdsks that fall in this time range
c  time_end                 default is (0.0, 1.e30)

c  rho_calc_method         0 or 1, default 1. 
c                          0 means use q to get rho
c                          1 means use toroidal flux to get rho
c  fit_rhomax_type
c  fit_psi_type           these are fitting specifiers for
c                         rhomax and psi
c                         possible values are
c                                "none"    eg no fit to data
c                                "linear"  eq do a linear fit
c                                "spline_icsvku"  !spline fit
c                                "spline_icsscv"  !spline fit
c                                see the IMSL manuals for the
c                                description of the spline fits
c  
c
c                          note that dpsidt_const_zeta(j,i), 
c                          dpsidt_const_rho(j,i),and 
c                          drhodt_const_zeta = rho_12(j)**drhomaxdt(i)
c                          are not calculated if fit_psi_type.eq. "none"
c                          It is then up to Onetwo to decide what will
c                          be done  to determine these values.
c
c
c
c  nk_rhomax               if fit_rhomax_type .eq. "spline_icsvku"
c                          then nk_rhomax is the number of knots
c
c  nk_psi                  if fit_psi_type .eq. "spline_icsvku"
c                          then nk_psi  is the number of knots


c  icsscv_ijob_psi          fit type parameter used only if 
                         fit_psi_type ="spline_icsscv"
c  icsscv_ijob_rhomax        fit type parameter used only if 
c                         fit_rhomax_type ="spline_icsscv"
c                          both of these are IJOB parameters:
c                         IJOB  parameter in icsscv call       
c                         from IMSL description:
c                       - JOB SELECTION PARAMETER. (INPUT)
c                        IJOB = 1 SHOULD BE SELECTED WHEN
c                          NX IS SMALL (LESS THAN ABOUT 20)
c                          OR WHEN UNEQUALLY SPACED ABSCISSAE
c                          (X(1),X(2),...) ARE USED.
c                        IJOB = 2 SHOULD BE SELECTED WHEN
c                          NX IS LARGE AND THE ABSCISSAE ARE
c                          EQUALLY SPACED.

c  pol_flux lim           normalized  value of poloidal flux to use
c                         for plasma boundary

c output_file_name        Name for netcdf file to be created


c plot_file_name          Name to use for cgm plot file output


c input_file_name          name of file that contains list of eqdsks to 
c                          process
c
c dump_file_name          All the data that was caluated will be dumped to this
c dum_values              ASCII file if dump_values = .true.
c
c
c
c----------------------------------
    \end{verbatim}

c  analysis\_check if =1   (which is the default) the
c                 profiles run in analysis mode are included in the
c                 check for the relative maximum change, see relmax.
c                 This means that the time step might be cut back
c                 even if we are not solving the corresponding
c                 diffusion equation. This is useful for checking the
c                 consistency of the time dependent profiles that are 
c                 specified in inone. If this effect is not wanted
c                  then set analysis\_check = 0

\section{Glf23 modeling}
      The two previous versions of GLF23  were removed from \ot and
      replaced with one  new routine. As uasual an input description
      of the available options that can be set in the \ot input
      file,inone, 
      is given in file cray102.f . There are several new options
      as follows:
     \begin{description}
       \item [glf23\_iglf] Determines if old (=0) or new (=1)
           version will be run. The ``old'' version is the one prior
           to Feb. 2003 which had some problems with reversed shear
           discharges. The new one is version 1.6 which is an intermin
           fix for reversed shear cases. (It is expected that version
           1.6 will eventualy be replaced by a more refined model)
           The code defaults to using version 1.6. 
       \item [write\_glf\_namelist] Valid settings are 0,1,2,3.
           0 means do not 
             write a namelist (in file glf23\_namelist) .
                   1 means do write the namelist.
                    This namelist is a convenient way of getting
                     a single time slice of information from onetwo
                    into the glf23 stand alone code (as released to
                    the NTCC). In that stand alone code the
                    executable testglf can read this namelist (after changeing
                    the name from "glf\_23\_namelist" to "in" 
                    and setting lprint = 1). In this mode only the
                    results from  the most current time are saved.
                    Note  that the file glf\_23\_namelist only
                    contains input for Glf23. The output from Glf23 is
                    obtained from the file witten by the testglf code
                    by setting lprint =1. (There is also a debug
                    option, see the switch glf\_debug in cray102.f,
                    that includes output from Glf23)
            write\_glf\_namelist = 2  means write the namelist and terminate Onetwo
                   right after the very first time that glf is called.
            write\_glf\_namelist =  3  write the namelist and terminate Onetwo
                   at the start of the inital time step.
                   This differs from the  write\_glf\_namelist =2 
                   option in that all sources (rf,beam)
                   are called at the initial time first, which changes
                   some input into glf such as the electron density
                   due to beam efects.
   \item [ glf23\_ncpus ]  The  number of cpus to use in Glf23 calculations.
                    Valid only on multiple cpu machines with a proper
                    intallation of mpi. At this time the messsage
                    passing overhead is too great (see the table
                    below) for this option to be  used.
     \end{description}

     The stiff non linear behavior of the GLF23 confinment model
     (and others sucah as Wiland)
     requires much more computational effort than previous models.
     To deal with this problem two additional solution methods were
     created in \ot. The first is an adaptive method of lines approach
     whereby only the spatial derivatives are replaced by finite difference
     forms. The resulting set of coupled equations are then ordinary
     differentail equations in time and can be solved by ``black box''
     ode solvers . \ot uses the Radau5 package whcih allows variable
     variable coefficients for the time derivatives. It was found that
     this approach 

     The second new solution method introduced into \ot is
      a globally convergent combiantion of steepest descent,trust
      region, and Newton type solvers using line search, dog leg and
      hook step methods (familiar from the numerical solution of sets of
      non linear equations). This arsenal is required to solve the
      resulting set of non linear coupled algebraic equations that are
      obtained by finite differencing both the space and time
      derivatives in the diffusion equations. One major advantage of
      this techmique is that it allows us to run both time dependent
      and time indepenent cases.In he time indepedent case we solve
      the diffusion equations with the time derivative explictely
      zeroed. The resulting set of non linear algebraic equations
       may not have a solution, reflecting the physical fact taht the
       current set of sources do not lead to a time indepdent
       solution.
       But our solution method is capabe of handling such situations by using
       descent methods whereby the residula sum of ssquares of all
       the equations  (one eqaution for each variable,and for each grid
       point) is minimized. Hence even if we do not find a solution
       converged to the usuall degree (typically < 1.e-8),we will
       still get the best approximate solution and a measure of how
       much this solution deviates from the expected true solution. 
       Quite often this is enough information to allow 
       manual modification of the problem in order to generate an
       acceptable result. An obvious
      application is the determiantion of steady state current drive 
    situations. Rather than evolve the system for 100 sec or more to
    relax  the current density sufficiently ( for Diii-D) we instead
    step directly to the equilibirum solution. (Of course it may take a
    signifcant number of iterations to solve even the time indepednet
    equations)
      
      The interested reader
      can find more information and examples 
      regarding these solution techniques in
      the detailed \ot writeup (\\u\\stjohn\\)
     
      Note that Glf23 assums that the diffusion equations are of the
      form

      with no convection. In \ot the corresponding equation is


      and hence we must apply the correction factor. This is done
      by solveing the equations with an effective chi given by

     in \ot. To compare diffusivities with Xptor we must keep
     this effect in mind. The output from \ot prints and plots the
     effective chi which will differ from the Xptor chi by the
     indicated factor.

\subsection{Parallel Glf23  Computations}
     A parallel version of the stand alone GLF23 test code was
     created to evaluate  the feasibility of using the distributed
     memory   message passing
     interface (MPI), or the shared memory Open MP scheme
     to speed up the calculations.
      Open MP has the huge  advantage over MPI of allowing
     incremental code parallelization  whereas MPI  requires that the
     entire code be parallelized before it can be used.
      Unfortunately our local computing
     environment is limited to 2 cpus for shared memory situations and hence 
    there is little to be gained in using Open MP. 
      Open Mp directives are   included in
    some relevant parts of \ot but no signifcant gain in execution
    time can be achieved using only 2 processors. (The 8 processor GS80
    shared memory machine ,gaws21.sd.gat.com, is the exception but
   it is not generally available for production \ot runs).

   To determine what could be done with MPI a 
   version of the stand alone Glf23 code was run with MPI 
  parallelization done  over the 51 point rho grid internal to
  subroutine callglf2d. The results for various combinations of
  machines and  processors is given in Table I.
    
  \begin{table}
      \begin{center}
     \begin{tabular}
      {cccc}
      No.  cpus & Taurus & Luna & Lohan1 \\ 
         & sec &sec &sec\\ \hline
        1 & .06 & 0.21&   \\
        2 & .14 & 0.21& \\
        4 &     & .13 & \\
        6 &     & .13 & \\
        8 &     & .11 & \\
       10 &     & .10 & \\
       12 &     & .11 & \\ 
       14 &     & .10 & \\
       16 &     & .10 & \\ \hline
    \end{tabular}
   \end{center}
     \caption{CPU Time for 51 Point GLF23 Calculation}
  \end{table}
   The message from  this table is pretty convincing. By comparing
  the single cpu results across machines we see that  commidity
   Athlon and Xeon processors  are operating at speeds that make
   it difficult for the slower  multiple cpu  machines to compete.
  The bottleneck is in communications amongst the processors. In this
  example there are two  places where communication is
 required. First, the master proces reads the input data and then
  broadcast the data  to all the slave processors. {The
 alternative would be to have copies of the input file on each slave
 machine and let each slave read the data directly. The reason that we
 dont do this is that it takes much longer to ship the file to each
 processor and read it than it does to broadcast it from a single
 processor.} Second, after each processor has done its part of the
calculations (in our case each processor has calculated the
diffusivities on some subset of the rho grid) the results have to be
collected and combined into a single set of arrays on the master process.
  The combined communications costs of these two bottlenecks 
  is so great in this example that even
  16 processors on Luna cannot beat the single processor results
 on Taurus. It was found  that the broadcast of the input data to all 
 the processors was very roughly constant at about 25 msec 
 for 8 or more processors.
 The  2 cpu case, where communication is localized, takes much less
 time of course ( ~ 5 msec) . The elapsed time to do the computations
 on the 51 point grid is about 100msec for 2 processors and decreases
 to a constant 20 msec for 10 or more processors. The final reduction
 of the data back to a single cpu (done  with multiple calls to
 MPI\_REDUCE) take the majority of the time and increases slowly from
 75 msec for 2 cpus to about 120 msec for the 16 cpu case. All of
 these timmings are on Luna and they fluctuate substantially as the
 system load  changes. However more precise averages are not needed to
 support the conclusions reached here. 
  Based on these observations we see that even if  Luna had faster
  processors the results would not improve greatly since it is the 
  data reduction phase( using MPI\_REDUCE)  that is the primary
  problem. Hence the Glf23
  computations are in fact not very well suited for parallelization
  on Luna at least. A faster communication mechanism is needed in
  order to improve this situation. 
  Alternatively we have to do more
 computations on each individual processor in order to increase the
 ratio  of computation to communication times. In the present
 example this would   be achieved by using 201 instead of 51 grid
 points. This would make the computational time comparable to the data
 collection time. 

      Based on these observations there is little incentive to pursue
 parallel computations of the Glf23 confinement model in \ot. Much
 larger sections of the code will have to be run in parallel before
 using MPI makes any sense. As far as \ot is concerned, the MPI
 approach  seems limited to
 Monte Carlo beam calculations and rf ray tracing, both massive,embarassingly
 parallel problems.   

     \section{Problems}
     Unfortunately there are hardware/compiler,software dependent problems that
     necessitate this section.
     
     Known problems are: \\
     (1) Preplt will not run on Katze due to insufficient memory.
         Onetwo will run  but you must select run\_preplt = .false.
         in the first namelist of inone. Note that if you do not do
         this Onetwo will take an error exit but the files will still
         be created. Hence you can move trpltfil to another machine
         and run preplt there.
     (2) The MEPC code uses Display for plotting which limits the
         places you can run it to Hydra and ??
     (3) The fastwave code current drive calculations fail when 
         nzrffw = 2 and nzrffw > 6 .
     (4) Currently the current drive calculations in Toray v1.4 do
         not agree when Toray is run on Hydra and Cardea
    \end{document}

 
#! /usr/bin/env bash
# GACODE Parallel execution script (PERLMUTTER_GPU)

#env 1>&2

if [ ${NTASKS_PER_GPU} -gt 1 ]; then

    let r4=SLURM_LOCALID/4
    let l4="$SLURM_LOCALID-($r4*4)"

    #echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID $ACC_DEVICE_NUM `taskset -pc $$`"

    #echo "$SLURM_PROCID `nvidia-smi -L` $SLURM_PROCID"

    # use MPS
    # https://docs.nvidia.com/deploy/mps/index.html
    #export CUDA_VISIBLE_DEVICES=0
    export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
    export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log
    
    # Store original visible GPUs
    export ORIGINAL_CUDA_VISIBLE=$CUDA_VISIBLE_DEVICES

    # Set GPU to only be visible to  associated tasks
    export visible=$(((SLURM_PROCID % (NUMA*NTASKS_PER_GPU)) / NTASKS_PER_GPU))
    export CUDA_VISIBLE_DEVICES=$visible
    
    NODE_RANK=$((SLURM_PROCID % (NTASKS_PER_GPU*NUMA)))
    
    if [ $NODE_RANK -eq 0 ]; then
	echo $SLURM_PROCID starting nvidia-cuda-mps-control on $(hostname)
	# Ensure host GPU can see all GPUs
	export CUDA_VISIBLE_DEVICES=$ORIGINAL_CUDA_VISIBLE
	nvidia-cuda-mps-control -d
    fi

    sleep 5
    
    if [ $NODE_RANK -eq 0 ]; then
	echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID mps ready: $(date --iso-8601=seconds)"
    fi
    
    "$@"
  
    if [ $NODE_RANK -eq 0 ]; then
	echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopping nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
	echo quit | nvidia-cuda-mps-control
	echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopped nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
    fi
else
    # no need for MPS

    # echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID $ACC_DEVICE_NUM `taskset -pc $$`"

    #echo "$SLURM_PROCID `nvidia-smi -L` $SLURM_PROCID"

    exec "$@"
    
fi


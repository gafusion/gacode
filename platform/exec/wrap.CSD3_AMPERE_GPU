#! /usr/bin/env bash
# GACODE Parallel execution script (PERLMUTTER_GPU)

export MPICH_GPU_SUPPORT_ENABLED=1

MEMS=(1 5 3 7)
DEVICE_NUM=(0 1 2 3)
CPUS=(48-63 16-31 112-127 80-95)

NUMBER=$(echo "`uname -n`" | tr -dc '0-9')

if [ $NUMBER -gt 80 ]; then
    NICS=(mlx5_0:1 mlx5_0:1 mlx5_3:1 mlx5_3:1)
else
    NICS=(mlx5_0:1 mlx5_0:1 mlx5_1:1 mlx5_1:1)
fi

echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID `taskset -pc $$`"

export OMP_NUM_THREADS=$NOMP

NTASKS_PER_NODE=$((SLURM_NTASKS / SLURM_NNODES))

if [ ${NTASKS_PER_NODE} -gt 4 ]; then
  # use MPS
  # https://docs.nvidia.com/deploy/mps/index.html
  export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
  export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log

  CUDA_MPS_PERCENTAGE=$((400 / NTASKS_PER_NODE))
  export CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=$CUDA_MPS_PERCENTAGE
  
  DEVICE_NUM=(1 0 2 3)

  NODE_RANK=$((SLURM_PROCID % NTASKS_PER_NODE))
  GPU_RANK=$((SLURM_PROCID % 4))

  export ACC_DEVICE_NUM=${DEVICE_NUM[${GPU_RANK}]}
  export UCX_NET_DEVICES=${NICS[${GPU_RANK}]}

  if [ $NODE_RANK -eq 0 ]; then
    echo $SLURM_PROCID starting nvidia-cuda-mps-control on $(hostname)
    nvidia-cuda-mps-control -d
  fi

  sleep 5

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID mps ready: $(date --iso-8601=seconds)"
  fi

  "$@"

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopping nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
    echo quit | nvidia-cuda-mps-control
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopped nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
  fi
else
    # no need for MPS
    export ACC_DEVICE_NUM=${DEVICE_NUM[${SLURM_LOCALID}]}
    export UCX_NET_DEVICES=${NICS[${SLURM_LOCALID}]}

    $@
fi


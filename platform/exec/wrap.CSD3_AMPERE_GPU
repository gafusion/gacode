#! /usr/bin/env bash
# GACODE Parallel execution script (PERLMUTTER_GPU)

export MPICH_GPU_SUPPORT_ENABLED=1

MEMS=(1 5 3 7)
DEVICE_NUM=(0 1 2 3)
CPUS=(48-63 16-31 112-127 80-95)
NICS=(mlx5_0:1 mlx5_0:1 mlx5_1:1 mlx5_1:1)

export ACC_DEVICE_NUM=${DEVICE_NUM[${SLURM_LOCALID}]}
export UCX_NET_DEVICES=${NICS[${SLURM_LOCALID}]}

echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID `taskset -pc $$`"

export OMP_NUM_THREADS=$NOMP

NTASKS_PER_NODE=$((SLURM_NTASKS / SLURM_NNODES))

if [ ${NTASKS_PER_NODE} -gt 4 ]; then
  # use MPS
  # https://docs.nvidia.com/deploy/mps/index.html
  export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
  export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log

  NODE_RANK=$((SLURM_PROCID % NTASKS_PER_NODE))

  if [ $NODE_RANK -eq 0 ]; then
    echo $SLURM_PROCID starting nvidia-cuda-mps-control on $(hostname)
    nvidia-cuda-mps-control -d
  fi

  sleep 5

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID mps ready: $(date --iso-8601=seconds)"
  fi

  "$@"

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopping nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
    echo quit | nvidia-cuda-mps-control
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopped nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
  fi
else
  # no need for MPS
  $@
fi

